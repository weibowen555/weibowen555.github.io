---
---

@string{aps = {American Physical Society,}}

@misc{wei2025learningexplainprototypebasedsurrogate,
      title={Learning to Explain: Prototype-Based Surrogate Models for LLM Classification}, 
      abbr={AAAI 2026 Oral},
      author={Bowen Wei and Mehrdad Fazli and Ziwei Zhu},
      year={2025},
      eprint={2505.18970},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      selected={true},
      url={https://arxiv.org/abs/2505.18970}, 
}

@inproceedings{wei-zhu-2025-protolens,
    title = "{P}roto{L}ens: Advancing Prototype Learning for Fine-Grained Interpretability in Text Classification",
    abbr={ACL 2025 Main},
    author = "Wei, Bowen  and
      Zhu, Ziwei",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.226/",
    doi = "10.18653/v1/2025.acl-long.226",
    pages = "4503--4523",
    ISBN = "979-8-89176-251-0",
    selected={true},
    abstract = "In this work, we propose ProtoLens, a novel prototype-based model that provides fine-grained, sub-sentence level interpretability for text classification. ProtoLens uses a Prototype-aware Span Extraction module to identify relevant text spans associated with learned prototypes and a Prototype Alignment mechanism to ensure prototypes are semantically meaningful throughout training. By aligning the prototype embeddings with human-understandable examples, ProtoLens provides interpretable predictions while maintaining competitive accuracy. Extensive experiments demonstrate that ProtoLens outperforms both prototype-based and non-interpretable baselines on multiple text classification benchmarks. Code and data are available at \url{https://github.com/weibowen555/ProtoLens}."
}

@misc{wei2025cortexcollaborativellmagents,
      title={CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage}, 
      abbr={NeurIPS LAW 2025},
      author={Bowen Wei and Yuan Shen Tay and Howard Liu and Jinhao Pan and Kun Luo and Ziwei Zhu and Chris Jordan},
      year={2025},
      eprint={2510.00311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.00311},
      selected={true}, 
}

@misc{fazli2025mitigatinghallucinationlargevisionlanguage,
      title={Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration}, 
      abbr={WACV 2026},
      author={Mehrdad Fazli and Bowen Wei and Ahmet Sari and Ziwei Zhu},
      year={2025},
      eprint={2505.21472},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.21472}, 
      selected={true},
}


@misc{wei2024neuralsymboliclogicalrule,
      title={Neural Symbolic Logical Rule Learner for Interpretable Learning}, 
      abbr={arXiv},
      author={Bowen Wei and Ziwei Zhu},
      year={2024},
      eprint={2408.11918},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.11918}, 
      selected={true},
}

@misc{raj2025vignettesociallygroundedbias,
      title={VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models}, 
      author={Chahat Raj and Bowen Wei and Aylin Caliskan and Antonios Anastasopoulos and Ziwei Zhu},
      abbr={arXiv},
      year={2025},
      eprint={2505.22897},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      selected={true},
      url={https://arxiv.org/abs/2505.22897}, 
}



@misc{
wei2023an,
title={An Empirical Study of the Neural Contextual Bandit Algorithms},
abbr={M.Sc. Thesis},
author={Bowen Wei and Yiling Jia and Hongning Wang},
year={2023},
url={https://openreview.net/forum?id=p4X5ZrM2AY}
}