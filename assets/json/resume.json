{"basics":{"name":"Bowen Wei","label":"Ph.D. Student in Computer Science","image":"","email":"bwei2@gmu.edu","phone":"+1 (434) 254-9053","url":"https://weibowen555.github.io/","summary":"Ph.D. student in Computer Science at George Mason University advised by Prof. Ziwei Zhu. My research focuses on trustworthy and interpretable AI for large language models, combining prototype-based, symbolic, and explanation-driven methods with reinforcement learning to enable verifiable, evidence-grounded reasoning.","location":{"address":"","postalCode":"22030","city":"Fairfax","countryCode":"US","region":"Virginia"},"profiles":[{"network":"LinkedIn","username":"bowen-wei-9485a1192","url":"https://www.linkedin.com/in/bowen-wei-9485a1192/"},{"network":"Website","username":"weibowen555","url":"https://weibowen555.github.io/"}]},"work":[{"name":"Fluency Security","position":"AI Agents Developer Intern","startDate":"2025-06-01","endDate":"2025-08-31","summary":"Developed multi-agent LLM systems for SOC alert triage; designed the CORTEX architecture for explainable collaboration among LLM agents.","highlights":["Proposed and built CORTEX multi-agent LLM framework for alert triage.","Curated analyst-trace dataset (10+ scenarios) with tool outputs.","Improved actionable F1 to 0.78 (+0.12) and reduced false-positive rate to 14.2%.","Paper accepted to NeurIPS 2025 LAW Workshop."]},{"name":"GoEngage","position":"GenAI Engineer Intern","startDate":"2025-06-01","endDate":"2025-08-31","summary":"Implemented semantic retrieval and built an agentic chatbot that interfaces with APIs to generate analytical reports.","highlights":["Implemented semantic search outperforming keyword baselines.","Built a reasoning-capable chatbot integrated with backend APIs."]}],"education":[{"institution":"George Mason University","location":"Fairfax, VA","url":"https://cs.gmu.edu/","area":"Computer Science","studyType":"Ph.D.","startDate":"2023-08-01","endDate":"2028-05-01","score":"","courses":["Trustworthy and Interpretable AI","Reinforcement Learning","Explainable LLMs"]},{"institution":"University of Virginia","location":"Charlottesville, VA","url":"https://engineering.virginia.edu/departments/computer-science","area":"Computer Science","studyType":"M.S.","startDate":"2021-08-01","endDate":"2023-05-01","score":"","courses":["Machine Learning","Neural Contextual Bandits"]},{"institution":"Xidian University","location":"Xi'an, China","url":"https://en.xidian.edu.cn/","area":"Computer Science","studyType":"B.S.","startDate":"2016-09-01","endDate":"2021-06-01","score":""}],"publications":[{"name":"Making Sense of LLM Decisions: A Prototype-based Framework for Explainable Classification","publisher":"AAAI 2026 (Oral)","releaseDate":"2026-02-01","summary":"Develops a prototype-based framework to explain classification decisions in large language models with transparent, interpretable reasoning.","url":""},{"name":"ProtoLens: Advancing Prototype Learning for Fine-Grained Interpretability in Text Classification","publisher":"ACL 2025 (Main)","releaseDate":"2025-07-01","summary":"Introduces ProtoLens, improving interpretability in text classification through prototype learning and fine-grained analysis of linguistic features.","url":""},{"name":"CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage","publisher":"NeurIPS 2025 LAW Workshop","releaseDate":"2025-12-01","summary":"Proposes a collaborative LLM-agent framework for security operations center alert triage with interpretable decision-making.","url":""},{"name":"CAAC: Confidence-Aware Attention Calibration to Reduce Hallucinations in Large Vision-Language Models","publisher":"WACV 2026","releaseDate":"2026-01-01","summary":"Presents an attention calibration method that mitigates hallucinations in large vision-language models by accounting for model confidence.","url":""},{"name":"Neural Symbolic Logical Rule Learner for Interpretable Learning","publisher":"ICLR 2026 (Under Review)","releaseDate":"2026-04-01","summary":"Combines neural representation learning with symbolic logical reasoning for transparent decision boundaries.","url":""},{"name":"VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models","publisher":"In Submission","releaseDate":"2026-03-01","summary":"Proposes a framework to evaluate and mitigate social bias in vision-language models through grounded scenarios.","url":""},{"name":"An Empirical Study of Neural Contextual Bandit Algorithms","publisher":"M.Sc. Thesis, University of Virginia","releaseDate":"2023-05-01","summary":"A comparative analysis of contextual bandit algorithms with neural approximators in recommendation and decision-making tasks.","url":""}],"projects":[{"name":"Evidence-Attribution Reinforcement Learning (EA-RL)","summary":"Develops reinforcement learning methods rewarding LLMs for using correct evidence, not only producing correct answers.","highlights":["Designs multi-agent distillation with explicit evidence attribution.","Builds on CoA for faithful, verifiable single-agent reasoning."],"startDate":"2025-10-01","endDate":"","url":""},{"name":"NeuroSymbolic Autoencoder for Interpretable Recommendation","summary":"Integrates neural and symbolic reasoning for explainable recommendation systems using rule-based latent spaces.","highlights":["Employs Rule Network as encoder-decoder for transparent RecSys.","Targets SIGIR 2026 for publication."],"startDate":"2025-10-01","endDate":"","url":""}],"skills":[{"name":"Machine Learning & AI","level":"Advanced","keywords":["Trustworthy AI","Interpretable Models","Prototype Learning","Reinforcement Learning","Neuro-Symbolic Learning","Large Language Models"]}],"languages":[{"language":"English","fluency":"Fluent"},{"language":"Chinese","fluency":"Native"}],"interests":[{"name":"Research in Explainable AI","keywords":["LLM Interpretability","Prototype-Based Reasoning","Neural-Symbolic Learning","Evidence-Grounded RL"]}],"awards":[],"volunteer":[],"certificates":[],"references":[]}